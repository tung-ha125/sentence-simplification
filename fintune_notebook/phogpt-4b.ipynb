{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8398345,"sourceType":"datasetVersion","datasetId":4692054},{"sourceId":8399217,"sourceType":"datasetVersion","datasetId":4626117}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip3 install -q -U bitsandbytes\n!pip3 install -q -U peft\n!pip3 install -q -U trl\n!pip3 install -q -U accelerate\n!pip3 install -q -U datasets==2.17.0\n!pip3 install -q git+https://github.com/huggingface/transformers.git\n!pip install einop","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-14T16:24:57.787050Z","iopub.execute_input":"2024-05-14T16:24:57.787955Z","iopub.status.idle":"2024-05-14T16:28:28.903684Z","shell.execute_reply.started":"2024-05-14T16:24:57.787918Z","shell.execute_reply":"2024-05-14T16:28:28.902393Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# from unsloth import FastLanguageModel\n# import torch\n# max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n# dtype = torch.float16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n# load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n# hf_token = \"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\"","metadata":{"execution":{"iopub.status.busy":"2024-05-14T16:28:28.905677Z","iopub.execute_input":"2024-05-14T16:28:28.906024Z","iopub.status.idle":"2024-05-14T16:28:28.910427Z","shell.execute_reply.started":"2024-05-14T16:28:28.905998Z","shell.execute_reply":"2024-05-14T16:28:28.909554Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nimport os\nimport numpy as np\nfrom peft import LoraConfig, AutoPeftModelForCausalLM\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig, TrainingArguments\nfrom trl import SFTTrainer\n\nmodel_id = \"vinai/PhoGPT-4B\"","metadata":{"execution":{"iopub.status.busy":"2024-05-14T16:28:28.911641Z","iopub.execute_input":"2024-05-14T16:28:28.911913Z","iopub.status.idle":"2024-05-14T16:28:45.420463Z","shell.execute_reply.started":"2024-05-14T16:28:28.911883Z","shell.execute_reply":"2024-05-14T16:28:45.419468Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-05-14 16:28:36.483643: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-14 16:28:36.483760: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-14 16:28:36.604795: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"def create_model_and_tokenizer():\n#     bnb_config = BitsAndBytesConfig(\n#         load_in_4bit=True,\n#         bnb_4bit_use_double_quant=True,\n#         bnb_4bit_quant_type=\"nf4\",\n#         bnb_4bit_compute_dtype=torch.float16\n#     )\n    config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id,\n        config=config,\n#         quantization_config=bnb_config, \n        device_map=\"auto\",\n        torch_dtype=torch.float16,\n#         trust_remote_code=True,\n        load_in_8bit=True\n    )\n    tokenizer.padding_side = \"right\"\n    model.config.use_cache = False\n    return model, tokenizer\n\nmodel, tokenizer = create_model_and_tokenizer()\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-05-14T16:28:45.422381Z","iopub.execute_input":"2024-05-14T16:28:45.422951Z","iopub.status.idle":"2024-05-14T16:29:51.231199Z","shell.execute_reply.started":"2024-05-14T16:28:45.422925Z","shell.execute_reply":"2024-05-14T16:29:51.230276Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69157bdb44444e53a136ee0b80bb61cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_mpt.py:   0%|          | 0.00/16.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d67d1e13c6647c39658944e69917c6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"attention.py:   0%|          | 0.00/24.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cea76022279454bace62e3366d82c2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"norm.py:   0%|          | 0.00/3.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"851195286ee147558d5ccb45cf8d5f32"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- norm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"flash_attn_triton.py:   0%|          | 0.00/28.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0128e3d09a0492c8ba25ea2fc191ffc"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- flash_attn_triton.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"fc.py:   0%|          | 0.00/167 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c9457bc82e849d49f6e1637dd474fba"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- fc.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- attention.py\n- norm.py\n- flash_attn_triton.py\n- fc.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"warnings.py:   0%|          | 0.00/894 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c20f91c4ac8f4091ab9d9b2ee5b605fe"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- warnings.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"blocks.py:   0%|          | 0.00/4.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f1999c7a9af43f486a819760c800c7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ffn.py:   0%|          | 0.00/5.22k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af95f82d675b470993e3a5099dcc3133"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- ffn.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- blocks.py\n- ffn.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- configuration_mpt.py\n- attention.py\n- warnings.py\n- blocks.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n/root/.cache/huggingface/modules/transformers_modules/vinai/PhoGPT-4B/9b1f40c891445071e999aa446ef7303883ee8935/configuration_mpt.py:114: UserWarning: alibi or rope is turned on, setting `learned_pos_emb` to `False.`\n  warnings.warn(f'alibi or rope is turned on, setting `learned_pos_emb` to `False.`')\n/root/.cache/huggingface/modules/transformers_modules/vinai/PhoGPT-4B/9b1f40c891445071e999aa446ef7303883ee8935/configuration_mpt.py:141: UserWarning: If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".\n  warnings.warn(UserWarning('If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".'))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/260 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d80c2ca9241e42ee8cca5c8be2312477"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/844k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ec0275a40004661bd35ef14de72eb98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89207a217389407792154326367f80c3"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for vinai/PhoGPT-4B contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/vinai/PhoGPT-4B.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"modeling_mpt.py:   0%|          | 0.00/32.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f948a14ef3f48cebcc4ec2a54485d57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"custom_embedding.py:   0%|          | 0.00/292 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08cef24616cc482a85b6c1e0652ed92c"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- custom_embedding.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"hf_prefixlm_converter.py:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80c6c94355614af2a22fb5352a4efd98"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- hf_prefixlm_converter.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"param_init_fns.py:   0%|          | 0.00/11.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5deb4371c9df4b78933f0677843166ff"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- param_init_fns.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapt_tokenizer.py:   0%|          | 0.00/1.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edcea40602ec4f869a190420f85ce13d"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- adapt_tokenizer.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"meta_init_context.py:   0%|          | 0.00/3.96k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adf94f4129e949b39869a06b0cf8ee3f"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- meta_init_context.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- modeling_mpt.py\n- custom_embedding.py\n- hf_prefixlm_converter.py\n- param_init_fns.py\n- adapt_tokenizer.py\n- meta_init_context.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nThe `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/7.38G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"728e2e422fb3495489d6f19514ff464a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/91.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa04067be5534547ace2cf66c5de84ef"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"MPTForCausalLM(\n  (transformer): MPTModel(\n    (wte): SharedEmbedding(20480, 3072)\n    (emb_drop): Dropout(p=0.0, inplace=False)\n    (blocks): ModuleList(\n      (0-31): 32 x MPTBlock(\n        (norm_1): LPLayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (Wqkv): Linear8bitLt(in_features=3072, out_features=9216, bias=True)\n          (out_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=True)\n        )\n        (norm_2): LPLayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n        (ffn): MPTMLP(\n          (up_proj): Linear8bitLt(in_features=3072, out_features=12288, bias=True)\n          (down_proj): Linear8bitLt(in_features=12288, out_features=3072, bias=True)\n        )\n        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n        (resid_ffn_dropout): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (norm_f): LPLayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"alpaca_prompt = \"\"\"D∆∞·ªõi ƒë√¢y l√† m·ªôt h∆∞·ªõng d·∫´n m√¥ t·∫£ m·ªôt nhi·ªám v·ª•, k√®m theo m·ªôt ƒë·∫ßu v√†o cung c·∫•p th√™m ng·ªØ c·∫£nh. Vi·∫øt m·ªôt ph·∫£n h·ªìi ho√†n th√†nh ƒë√∫ng y√™u c·∫ßu.  \n\n### H∆∞·ªõng d·∫´n: \n{}  \n\n### ƒê·∫ßu v√†o: \n{}  \n\n### Ph·∫£n h·ªìi: \n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ninstruction = \"\"\"H√£y ƒë∆°n gi·∫£n h√≥a c√¢u sau d·ª±a tr√™n th√¥ng tin m√† t√¥i cung c·∫•p.\nƒê·ªãnh nghƒ©a: ƒê∆°n gi·∫£n h√≥a c√¢u bao g·ªìm vi·ªác s·ª≠a ƒë·ªïi n·ªôi dung v√† c·∫•u tr√∫c c·ªßa m·ªôt c√¢u ƒë·ªÉ l√†m cho n√≥ d·ªÖ hi·ªÉu h∆°n, trong khi v·∫´n gi·ªØ l·∫°i √Ω ch√≠nh v√† h·∫ßu h·∫øt nghƒ©a ban ƒë·∫ßu c·ªßa n√≥.\nƒê·ªÉ ƒë∆°n gi·∫£n h√≥a m·ªôt c√¢u, c√≥ th·ªÉ th·ª±c hi·ªán m·ªôt s·ªë ph√©p bi·∫øn ƒë·ªïi d∆∞·ªõi ƒë√¢y:\n1. Lo·∫°i b·ªè th√¥ng tin kh√¥ng c·∫ßn thi·∫øt.\n2. Lo·∫°i b·ªè ho·∫∑c gi·∫£m thi·ªÉu c√°c c·ª•m t·ª´ kh√¥ng mang l·∫°i nhi·ªÅu gi√° tr·ªã th√¥ng tin cho c√¢u.\n3. Thay th·∫ø c√°c t·ª´ / c·ª•m t·ª´ ph·ª©c t·∫°p b·∫±ng c√°c t·ª´ ƒë·ªìng nghƒ©a ƒë∆°n gi·∫£n h∆°n.\n4. Chia c√¢u ph·ª©c t·∫°p, nhi·ªÅu th√¥ng tin th√†nh c√°c c√¢u nh·ªè h∆°n.\nL∆∞u √Ω r·∫±ng c√¢u ƒë∆°n gi·∫£n kh√¥ng ch·ª©a b·∫•t k·ª≥ th√¥ng tin n√†o kh√¥ng c√≥ ho·∫∑c kh√¥ng ƒë√∫ng v·ªõi c√¢u g·ªëc.\n\"\"\".strip()\ndef formatting_prompts_func(examples):\n    # instructions = instruction\n    inputs       = examples[\"sentence\"]\n    outputs      = examples[\"result\"]\n    texts = []\n    for input, output in zip(inputs, outputs):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\n\nfrom datasets import load_dataset\ndata_files = {\n    \"train\": \"train.json\",\n    \"eval\": \"eval.json\"\n}\n\ndataset = load_dataset(\"/kaggle/input/ss-dataset\", data_files=data_files)\ndataset = dataset.map(formatting_prompts_func, batched = True,)\ndataset[\"train\"][0]","metadata":{"execution":{"iopub.status.busy":"2024-05-14T16:29:51.232313Z","iopub.execute_input":"2024-05-14T16:29:51.232588Z","iopub.status.idle":"2024-05-14T16:29:56.052971Z","shell.execute_reply.started":"2024-05-14T16:29:51.232564Z","shell.execute_reply":"2024-05-14T16:29:56.052076Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7e94545b8d5422590e1e99f6a955d7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating eval split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b38b726f38846438a8fb7fe6c8edd60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9157 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cc619b274ee4f51b7e743a820f16f22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df36bd85833244b4a9d6d848e9800a47"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'sentence': 'Ai c≈©ng bi·∫øt , \" ƒë·ªãnh m·ªánh \" v√† \" th·∫£m h·ªça \" l√† do Soneca v√† nh·ªØng ng∆∞·ªùi ch√¢u √Çu b·ªãa ra ; v·ªõi nh·ªØng ng∆∞·ªùi Hy L·∫°p , \" th·∫£m h·ªça \" l√† b∆∞·ªõc ngo·∫∑t quay ng∆∞·ª£c l·∫°i , ƒë·∫£o xu·ªëng ph√≠a d∆∞·ªõi , d·∫´u v·∫≠y , n√≥ c≈©ng ch·ªâ l√† \" b∆∞·ªõc ngo·∫∑t \" , c√≥ nghƒ©a , n√≥ l√† s·ª± chuy·ªÉn ƒë·ªông theo v√≤ng tr√≤n , t·ªõi l∆∞·ª£t sau n√≥ l·∫°i ƒë·∫£o l√™n ph√≠a tr√™n .',\n 'id': 'difficult_102_5',\n 'result': 'M·ªçi ng∆∞·ªùi ƒë·ªÅu bi·∫øt, Soneca v√† ng∆∞·ªùi ch√¢u √Çu t·∫°o ra \"ƒë·ªãnh m·ªánh\" v√† \"th·∫£m h·ªça\". ƒê·ªëi v·ªõi ng∆∞·ªùi Hy L·∫°p, \"th·∫£m h·ªça\" ch·ªâ l√† m·ªôt s·ª± thay ƒë·ªïi, m·ªôt s·ª± l·∫≠t ng∆∞·ª£c. N√≥i c√°ch kh√°c, n√≥ ch·ªâ l√† m·ªôt s·ª± chuy·ªÉn ƒë·ªông v√≤ng tr√≤n, sau ƒë√≥ n√≥ s·∫Ω quay tr·ªü l·∫°i.',\n 'text': 'D∆∞·ªõi ƒë√¢y l√† m·ªôt h∆∞·ªõng d·∫´n m√¥ t·∫£ m·ªôt nhi·ªám v·ª•, k√®m theo m·ªôt ƒë·∫ßu v√†o cung c·∫•p th√™m ng·ªØ c·∫£nh. Vi·∫øt m·ªôt ph·∫£n h·ªìi ho√†n th√†nh ƒë√∫ng y√™u c·∫ßu.  \\n\\n### H∆∞·ªõng d·∫´n: \\nH√£y ƒë∆°n gi·∫£n h√≥a c√¢u sau d·ª±a tr√™n th√¥ng tin m√† t√¥i cung c·∫•p.\\nƒê·ªãnh nghƒ©a: ƒê∆°n gi·∫£n h√≥a c√¢u bao g·ªìm vi·ªác s·ª≠a ƒë·ªïi n·ªôi dung v√† c·∫•u tr√∫c c·ªßa m·ªôt c√¢u ƒë·ªÉ l√†m cho n√≥ d·ªÖ hi·ªÉu h∆°n, trong khi v·∫´n gi·ªØ l·∫°i √Ω ch√≠nh v√† h·∫ßu h·∫øt nghƒ©a ban ƒë·∫ßu c·ªßa n√≥.\\nƒê·ªÉ ƒë∆°n gi·∫£n h√≥a m·ªôt c√¢u, c√≥ th·ªÉ th·ª±c hi·ªán m·ªôt s·ªë ph√©p bi·∫øn ƒë·ªïi d∆∞·ªõi ƒë√¢y:\\n1. Lo·∫°i b·ªè th√¥ng tin kh√¥ng c·∫ßn thi·∫øt.\\n2. Lo·∫°i b·ªè ho·∫∑c gi·∫£m thi·ªÉu c√°c c·ª•m t·ª´ kh√¥ng mang l·∫°i nhi·ªÅu gi√° tr·ªã th√¥ng tin cho c√¢u.\\n3. Thay th·∫ø c√°c t·ª´ / c·ª•m t·ª´ ph·ª©c t·∫°p b·∫±ng c√°c t·ª´ ƒë·ªìng nghƒ©a ƒë∆°n gi·∫£n h∆°n.\\n4. Chia c√¢u ph·ª©c t·∫°p, nhi·ªÅu th√¥ng tin th√†nh c√°c c√¢u nh·ªè h∆°n.\\nL∆∞u √Ω r·∫±ng c√¢u ƒë∆°n gi·∫£n kh√¥ng ch·ª©a b·∫•t k·ª≥ th√¥ng tin n√†o kh√¥ng c√≥ ho·∫∑c kh√¥ng ƒë√∫ng v·ªõi c√¢u g·ªëc.  \\n\\n### ƒê·∫ßu v√†o: \\nAi c≈©ng bi·∫øt , \" ƒë·ªãnh m·ªánh \" v√† \" th·∫£m h·ªça \" l√† do Soneca v√† nh·ªØng ng∆∞·ªùi ch√¢u √Çu b·ªãa ra ; v·ªõi nh·ªØng ng∆∞·ªùi Hy L·∫°p , \" th·∫£m h·ªça \" l√† b∆∞·ªõc ngo·∫∑t quay ng∆∞·ª£c l·∫°i , ƒë·∫£o xu·ªëng ph√≠a d∆∞·ªõi , d·∫´u v·∫≠y , n√≥ c≈©ng ch·ªâ l√† \" b∆∞·ªõc ngo·∫∑t \" , c√≥ nghƒ©a , n√≥ l√† s·ª± chuy·ªÉn ƒë·ªông theo v√≤ng tr√≤n , t·ªõi l∆∞·ª£t sau n√≥ l·∫°i ƒë·∫£o l√™n ph√≠a tr√™n .  \\n\\n### Ph·∫£n h·ªìi: \\nM·ªçi ng∆∞·ªùi ƒë·ªÅu bi·∫øt, Soneca v√† ng∆∞·ªùi ch√¢u √Çu t·∫°o ra \"ƒë·ªãnh m·ªánh\" v√† \"th·∫£m h·ªça\". ƒê·ªëi v·ªõi ng∆∞·ªùi Hy L·∫°p, \"th·∫£m h·ªça\" ch·ªâ l√† m·ªôt s·ª± thay ƒë·ªïi, m·ªôt s·ª± l·∫≠t ng∆∞·ª£c. N√≥i c√°ch kh√°c, n√≥ ch·ªâ l√† m·ªôt s·ª± chuy·ªÉn ƒë·ªông v√≤ng tr√≤n, sau ƒë√≥ n√≥ s·∫Ω quay tr·ªü l·∫°i.</s>'}"},"metadata":{}}]},{"cell_type":"code","source":"_max = -1\nfor i in dataset[\"train\"][\"text\"]:\n    arr = tokenizer.encode(i)\n    length = len(arr)\n    if length > _max:\n        _max = length\nprint(_max)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T14:00:42.948795Z","iopub.execute_input":"2024-05-14T14:00:42.949069Z","iopub.status.idle":"2024-05-14T14:00:51.661240Z","shell.execute_reply.started":"2024-05-14T14:00:42.949044Z","shell.execute_reply":"2024-05-14T14:00:51.660314Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"909\n","output_type":"stream"}]},{"cell_type":"code","source":"import wandb\nimport os\n\nwandb.login()\n%env WANDB_PROJECT=ss-v1\n%env WANDB_LOG_MODEL=false\n\n# os.environ[\"WANDB_API_KEY\"] = \"764d0f82f6d2b1cbdc302d0e476f0cbddc6a6033\"\n# os.environ[\"WANDB_PROJECT\"]=\"ss-finetune-v2\"","metadata":{"execution":{"iopub.status.busy":"2024-05-14T16:29:56.054130Z","iopub.execute_input":"2024-05-14T16:29:56.054403Z","iopub.status.idle":"2024-05-14T16:30:07.296594Z","shell.execute_reply.started":"2024-05-14T16:29:56.054380Z","shell.execute_reply":"2024-05-14T16:30:07.295762Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"env: WANDB_PROJECT=ss-v1\nenv: WANDB_LOG_MODEL=false\n","output_type":"stream"}]},{"cell_type":"code","source":"r = 16\na = r * 2\nlora_kwargs = {\n    \"r\": r,\n    \"lora_alpha\": a,\n    \"target_modules\": [\n        \"Wqkv\",\n        \"out_proj\",\n        \"up_proj\",\n        \"down_proj\",\n    ],\n    \"lora_dropout\": 0.1,\n    \"bias\": \"none\",\n    \"task_type\": \"CAUSAL_LM\"\n}\nconfig = LoraConfig(**lora_kwargs)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T16:30:10.028563Z","iopub.execute_input":"2024-05-14T16:30:10.029287Z","iopub.status.idle":"2024-05-14T16:30:10.036692Z","shell.execute_reply.started":"2024-05-14T16:30:10.029257Z","shell.execute_reply":"2024-05-14T16:30:10.035747Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n    model = model,\n    peft_config=config,\n    tokenizer = tokenizer,\n    train_dataset = dataset[\"train\"],\n    eval_dataset = dataset[\"eval\"],\n    dataset_text_field = \"text\",\n    max_seq_length = 1024,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 1,\n        gradient_accumulation_steps = 64,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        # max_steps = 60,\n        num_train_epochs=2,\n        learning_rate = 2e-4,\n#         fp16 = True,\n#         bf16 = False,\n        logging_steps = 1,\n        optim = \"adamw_torch\",\n        lr_scheduler_type = \"constant\",\n        seed = 3407,\n        output_dir = \"phogpt4b-r16a32-lr2e-4-adamw32bit-dr0.1\",\n        run_name = \"phogpt4b-r16a32-lr2e-4-adamw32bit-dr0.1\",\n        report_to=\"wandb\"\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T16:30:12.387929Z","iopub.execute_input":"2024-05-14T16:30:12.388767Z","iopub.status.idle":"2024-05-14T16:30:18.976700Z","shell.execute_reply.started":"2024-05-14T16:30:12.388733Z","shell.execute_reply":"2024-05-14T16:30:18.975663Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/9157 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3efbed616dcc4bc6a446f6761b7b13fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f793d26733614769b3de87cc94983c97"}},"metadata":{}}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T16:30:18.978610Z","iopub.execute_input":"2024-05-14T16:30:18.978918Z","iopub.status.idle":"2024-05-15T00:22:30.349073Z","shell.execute_reply.started":"2024-05-14T16:30:18.978892Z","shell.execute_reply":"2024-05-15T00:22:30.347991Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhasontung\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_163019-zw3cmvbx</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hasontung/ss-v1/runs/zw3cmvbx' target=\"_blank\">phogpt4b-r16a32-lr2e-4-adamw32bit-dr0.1</a></strong> to <a href='https://wandb.ai/hasontung/ss-v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hasontung/ss-v1' target=\"_blank\">https://wandb.ai/hasontung/ss-v1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hasontung/ss-v1/runs/zw3cmvbx' target=\"_blank\">https://wandb.ai/hasontung/ss-v1/runs/zw3cmvbx</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/root/.cache/huggingface/modules/transformers_modules/vinai/PhoGPT-4B/9b1f40c891445071e999aa446ef7303883ee8935/configuration_mpt.py:114: UserWarning: alibi or rope is turned on, setting `learned_pos_emb` to `False.`\n  warnings.warn(f'alibi or rope is turned on, setting `learned_pos_emb` to `False.`')\n/root/.cache/huggingface/modules/transformers_modules/vinai/PhoGPT-4B/9b1f40c891445071e999aa446ef7303883ee8935/configuration_mpt.py:141: UserWarning: If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".\n  warnings.warn(UserWarning('If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".'))\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/root/.cache/huggingface/modules/transformers_modules/vinai/PhoGPT-4B/9b1f40c891445071e999aa446ef7303883ee8935/attention.py:87: UserWarning: Propagating key_padding_mask to the attention module and applying it within the attention module can cause unnecessary computation/memory usage. Consider integrating into attn_bias once and passing that to each attention module instead.\n  warnings.warn('Propagating key_padding_mask to the attention module ' + 'and applying it within the attention module can cause ' + 'unnecessary computation/memory usage. Consider integrating ' + 'into attn_bias once and passing that to each attention ' + 'module instead.')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='286' max='286' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [286/286 7:50:04, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.647600</td>\n      <td>0.557921</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.624300</td>\n      <td>0.545048</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/root/.cache/huggingface/modules/transformers_modules/vinai/PhoGPT-4B/9b1f40c891445071e999aa446ef7303883ee8935/attention.py:87: UserWarning: Propagating key_padding_mask to the attention module and applying it within the attention module can cause unnecessary computation/memory usage. Consider integrating into attn_bias once and passing that to each attention module instead.\n  warnings.warn('Propagating key_padding_mask to the attention module ' + 'and applying it within the attention module can cause ' + 'unnecessary computation/memory usage. Consider integrating ' + 'into attn_bias once and passing that to each attention ' + 'module instead.')\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/root/.cache/huggingface/modules/transformers_modules/vinai/PhoGPT-4B/9b1f40c891445071e999aa446ef7303883ee8935/configuration_mpt.py:114: UserWarning: alibi or rope is turned on, setting `learned_pos_emb` to `False.`\n  warnings.warn(f'alibi or rope is turned on, setting `learned_pos_emb` to `False.`')\n/root/.cache/huggingface/modules/transformers_modules/vinai/PhoGPT-4B/9b1f40c891445071e999aa446ef7303883ee8935/configuration_mpt.py:141: UserWarning: If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".\n  warnings.warn(UserWarning('If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".'))\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=286, training_loss=0.7191455841481269, metrics={'train_runtime': 28330.9335, 'train_samples_per_second': 0.646, 'train_steps_per_second': 0.01, 'total_flos': 1.3574178278693683e+17, 'train_loss': 0.7191455841481269, 'epoch': 1.9989079392814242})"},"metadata":{}}]},{"cell_type":"code","source":"dataset[\"eval\"] = dataset[\"eval\"].shuffle(seed=42)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T00:22:39.604255Z","iopub.execute_input":"2024-05-15T00:22:39.604614Z","iopub.status.idle":"2024-05-15T00:22:39.617383Z","shell.execute_reply.started":"2024-05-15T00:22:39.604584Z","shell.execute_reply":"2024-05-15T00:22:39.616508Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import torch\ndef summarize(model, text, decoding_strategy=\"greedy\"):\n#     FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n    inputs = tokenizer(\n    [\n        alpaca_prompt.format(\n            instruction, # instruction\n            text, # input\n            \"\", # output - leave this blank for generation!\n        )\n    ], return_tensors = \"pt\").to(\"cuda\")\n    inputs_length = len(inputs[\"input_ids\"][0])\n    \n    generation_params = None\n    if decoding_strategy == \"greedy\":\n        generation_params = {\n            \"max_new_tokens\": 300,\n            \"top_k\": 1,\n        }\n    elif decoding_strategy == \"beam\":\n        generation_params = {\n            \"max_new_tokens\": 300,\n            \"num_beams\": 5,\n            \"early_stopping\": True,\n        }\n    elif decoding_strategy == \"sampling\":\n        generation_params = {\n            \"max_new_tokens\": 300,\n            \"do_sample\": True,\n            \"top_p\": 0.95,\n            \"top_k\": 5,\n            \"temperature\": 0.5,\n        }\n    else:\n        print(\"Unknown decoding strategy.\")\n        return None;\n    with torch.inference_mode():\n        outputs = model.generate(\n            **inputs, \n            eos_token_id=tokenizer.eos_token_id,  \n            pad_token_id=tokenizer.pad_token_id,\n            use_cache=True,\n            **generation_params\n        )\n    del inputs\n    torch.cuda.empty_cache()\n    return tokenizer.decode(outputs[0][inputs_length:])","metadata":{"execution":{"iopub.status.busy":"2024-05-15T00:22:39.619225Z","iopub.execute_input":"2024-05-15T00:22:39.619557Z","iopub.status.idle":"2024-05-15T00:22:39.630685Z","shell.execute_reply.started":"2024-05-15T00:22:39.619522Z","shell.execute_reply":"2024-05-15T00:22:39.629583Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"sent = dataset[\"eval\"][\"sentence\"][200]\nprint(alpaca_prompt.format(\n        instruction, # instruction\n        sent, # input\n        \"\", # output - leave this blank for generation!\n    ))\nprint(summarize(model, sent, \"greedy\"))","metadata":{"execution":{"iopub.status.busy":"2024-05-15T00:24:07.236308Z","iopub.execute_input":"2024-05-15T00:24:07.236671Z","iopub.status.idle":"2024-05-15T00:24:13.163897Z","shell.execute_reply.started":"2024-05-15T00:24:07.236641Z","shell.execute_reply":"2024-05-15T00:24:13.162710Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"D∆∞·ªõi ƒë√¢y l√† m·ªôt h∆∞·ªõng d·∫´n m√¥ t·∫£ m·ªôt nhi·ªám v·ª•, k√®m theo m·ªôt ƒë·∫ßu v√†o cung c·∫•p th√™m ng·ªØ c·∫£nh. Vi·∫øt m·ªôt ph·∫£n h·ªìi ho√†n th√†nh ƒë√∫ng y√™u c·∫ßu.  \n\n### H∆∞·ªõng d·∫´n: \nH√£y ƒë∆°n gi·∫£n h√≥a c√¢u sau d·ª±a tr√™n th√¥ng tin m√† t√¥i cung c·∫•p.\nƒê·ªãnh nghƒ©a: ƒê∆°n gi·∫£n h√≥a c√¢u bao g·ªìm vi·ªác s·ª≠a ƒë·ªïi n·ªôi dung v√† c·∫•u tr√∫c c·ªßa m·ªôt c√¢u ƒë·ªÉ l√†m cho n√≥ d·ªÖ hi·ªÉu h∆°n, trong khi v·∫´n gi·ªØ l·∫°i √Ω ch√≠nh v√† h·∫ßu h·∫øt nghƒ©a ban ƒë·∫ßu c·ªßa n√≥.\nƒê·ªÉ ƒë∆°n gi·∫£n h√≥a m·ªôt c√¢u, c√≥ th·ªÉ th·ª±c hi·ªán m·ªôt s·ªë ph√©p bi·∫øn ƒë·ªïi d∆∞·ªõi ƒë√¢y:\n1. Lo·∫°i b·ªè th√¥ng tin kh√¥ng c·∫ßn thi·∫øt.\n2. Lo·∫°i b·ªè ho·∫∑c gi·∫£m thi·ªÉu c√°c c·ª•m t·ª´ kh√¥ng mang l·∫°i nhi·ªÅu gi√° tr·ªã th√¥ng tin cho c√¢u.\n3. Thay th·∫ø c√°c t·ª´ / c·ª•m t·ª´ ph·ª©c t·∫°p b·∫±ng c√°c t·ª´ ƒë·ªìng nghƒ©a ƒë∆°n gi·∫£n h∆°n.\n4. Chia c√¢u ph·ª©c t·∫°p, nhi·ªÅu th√¥ng tin th√†nh c√°c c√¢u nh·ªè h∆°n.\nL∆∞u √Ω r·∫±ng c√¢u ƒë∆°n gi·∫£n kh√¥ng ch·ª©a b·∫•t k·ª≥ th√¥ng tin n√†o kh√¥ng c√≥ ho·∫∑c kh√¥ng ƒë√∫ng v·ªõi c√¢u g·ªëc.  \n\n### ƒê·∫ßu v√†o: \nTh·ªânh tho·∫£ng c≈©ng c√≥ m·ªôt v√†i ho√†ng t·ª≠ nghe k·ªÉ v·ªÅ c√¢u chuy·ªán truy·ªÅn thuy·∫øt ·∫•y ƒë√£ chui v√†o b·ª•i h·ªìng gai t√¨m c√°ch v√†o l√¢u ƒë√†i , nh∆∞ng b·ª•i gai nh∆∞ c√≥ tay gi·ªØ ch·∫∑t h·ªç l·∫°i , khi·∫øn h·ªç b·ªã m·∫Øc ngh·∫Ωn .  \n\n### Ph·∫£n h·ªìi: \n\nC√≥ m·ªôt s·ªë ho√†ng t·ª≠ nghe k·ªÉ v·ªÅ c√¢u chuy·ªán n√†y v√† mu·ªën v√†o l√¢u ƒë√†i, nh∆∞ng b·ª•i gai gi·ªØ h·ªç l·∫°i, khi·∫øn h·ªç b·ªã m·∫Øc k·∫πt.</s>\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm import tqdm\nout_data = []\n# length = int(len(test_df) / 3) + 1 if len(test_df) % 3 != 0 else len(test_df) / 3\nwith tqdm(total=len(dataset[\"eval\"][\"sentence\"])) as pbar:\n    for i in range(len(dataset[\"eval\"][\"sentence\"])):\n        greedy_response = summarize(model, dataset[\"eval\"][\"sentence\"][i], \"greedy\").replace(tokenizer.eos_token, \"\").strip()\n#         sampling_response = summarize(model, example.prompt, \"sampling\").replace(\"<eos>\", \"\")\n#         beam_response = summarize(model, example.prompt, \"beam\").replace(\"<eos>\", \"\")\n        temp = {\n            \"sentence\": dataset[\"eval\"][\"sentence\"][i],\n            \"result\": dataset[\"eval\"][\"result\"][i],\n            \"pred\": greedy_response,\n#             \"sampling\": sampling_response,\n#             \"beam\": beam_response,\n        }\n        out_data.append(temp)\n        pbar.update(1)\nprint(out_data[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-15T00:24:29.865672Z","iopub.execute_input":"2024-05-15T00:24:29.866506Z","iopub.status.idle":"2024-05-15T01:28:53.400771Z","shell.execute_reply.started":"2024-05-15T00:24:29.866474Z","shell.execute_reply":"2024-05-15T01:28:53.399663Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [1:04:23<00:00,  7.73s/it]","output_type":"stream"},{"name":"stdout","text":"{'sentence': '·ªû ph·∫ßn sau c·ªßa b√†i vi·∫øt , ch√∫ng t√¥i s·∫Ω ƒëi v√†o tr√¨nh b√†y l·∫°i m·ªôt s·ªë th·ª±c h√†nh ƒë·ªÉ c√≥ th·ªÉ quan s√°t r√µ h∆°n c√°ch ƒë·∫∑t v·∫•n ƒë·ªÅ v√† c√°c thao t√°c trong nghi√™n c·ª©u .', 'result': 'Trong ph·∫ßn ti·∫øp theo, ch√∫ng t√¥i s·∫Ω gi·ªõi thi·ªáu m·ªôt s·ªë th·ª±c h√†nh gi√∫p hi·ªÉu r√µ h∆°n v·ªÅ c√°ch ƒë·∫∑t v·∫•n ƒë·ªÅ v√† thao t√°c nghi√™n c·ª©u.', 'pred': '·ªû ph·∫ßn sau, ch√∫ng t√¥i s·∫Ω n√≥i v·ªÅ c√°ch ƒë·∫∑t v·∫•n ƒë·ªÅ v√† c√°ch l√†m vi·ªác trong nghi√™n c·ª©u.'}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"dict_result = {\n    \"training_info\": {\n        \"model\": \"phogpt4b\",\n        \"detail\": \"r16 a32 optim torchadamw lr2e-4 dr0.1 2eps\"\n    },\n    \"result\": out_data\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-15T01:28:53.402935Z","iopub.execute_input":"2024-05-15T01:28:53.403355Z","iopub.status.idle":"2024-05-15T01:28:53.409174Z","shell.execute_reply.started":"2024-05-15T01:28:53.403320Z","shell.execute_reply":"2024-05-15T01:28:53.408153Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import json\n\ndef load_json(file_path):\n  with open(file_path, 'r', encoding='utf-8') as json_file:\n    content = json.load(json_file)\n  return content\n\ndef save_json(file_path, data):\n  with open(file_path, 'w', encoding='utf-8') as json_file:\n    json.dump(data, json_file, ensure_ascii=False, indent=2)\n\nsave_json(f\"/kaggle/working/new-ss-phogpt4b-r16a32-lr2e-4-adamw32bit-dr0.1\", dict_result)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T01:28:53.410323Z","iopub.execute_input":"2024-05-15T01:28:53.410586Z","iopub.status.idle":"2024-05-15T01:28:53.432498Z","shell.execute_reply.started":"2024-05-15T01:28:53.410564Z","shell.execute_reply":"2024-05-15T01:28:53.431200Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}