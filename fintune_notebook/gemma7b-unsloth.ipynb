{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8398345,"sourceType":"datasetVersion","datasetId":4692054},{"sourceId":8399217,"sourceType":"datasetVersion","datasetId":4626117},{"sourceId":8406636,"sourceType":"datasetVersion","datasetId":5002570}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install -U xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install \"unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git\"\n\n# Temporary fix for https://github.com/huggingface/datasets/issues/6753\n!pip install datasets==2.16.0 fsspec==2023.10.0 gcsfs==2023.10.0\n\n# !pip install transformers --upgrade","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-15T01:30:29.733856Z","iopub.execute_input":"2024-05-15T01:30:29.734305Z","iopub.status.idle":"2024-05-15T01:34:19.668597Z","shell.execute_reply.started":"2024-05-15T01:30:29.734261Z","shell.execute_reply":"2024-05-15T01:34:19.667308Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\ndtype = torch.float16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\nhf_token = \"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\"","metadata":{"execution":{"iopub.status.busy":"2024-05-15T01:44:22.309697Z","iopub.execute_input":"2024-05-15T01:44:22.310469Z","iopub.status.idle":"2024-05-15T01:44:31.428633Z","shell.execute_reply.started":"2024-05-15T01:44:22.310432Z","shell.execute_reply":"2024-05-15T01:44:31.427532Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-05-15 01:44:28.966768: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-15 01:44:28.966832: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-15 01:44:28.968513: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"model, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/gemma-7b-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    token = hf_token, # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T01:44:31.431000Z","iopub.execute_input":"2024-05-15T01:44:31.431594Z","iopub.status.idle":"2024-05-15T01:44:39.842815Z","shell.execute_reply.started":"2024-05-15T01:44:31.431564Z","shell.execute_reply":"2024-05-15T01:44:39.841848Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"==((====))==  Unsloth: Fast Gemma patching release 2024.5\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = False.\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","output_type":"stream"},{"name":"stderr","text":"Gemma's activation function should be approximate GeLU and not exact GeLU.\nChanging the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n","output_type":"stream"}]},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 32,\n    lora_dropout = 0.1, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T01:44:39.844395Z","iopub.execute_input":"2024-05-15T01:44:39.844714Z","iopub.status.idle":"2024-05-15T01:44:40.812750Z","shell.execute_reply.started":"2024-05-15T01:44:39.844684Z","shell.execute_reply":"2024-05-15T01:44:40.811670Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\nUnsloth will patch all other layers, except LoRA matrices, causing a performance hit.\nUnsloth 2024.5 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n","output_type":"stream"}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-05-15T01:44:40.814242Z","iopub.execute_input":"2024-05-15T01:44:40.814648Z","iopub.status.idle":"2024-05-15T01:44:40.840899Z","shell.execute_reply.started":"2024-05-15T01:44:40.814611Z","shell.execute_reply":"2024-05-15T01:44:40.839684Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): GemmaForCausalLM(\n      (model): GemmaModel(\n        (embed_tokens): Embedding(256000, 3072)\n        (layers): ModuleList(\n          (0-27): 28 x GemmaDecoderLayer(\n            (self_attn): GemmaSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (rotary_emb): GemmaFixedRotaryEmbedding()\n            )\n            (mlp): GemmaMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=24576, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=24576, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=24576, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=24576, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=24576, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=24576, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n              )\n              (act_fn): PytorchGELUTanh()\n            )\n            (input_layernorm): GemmaRMSNorm()\n            (post_attention_layernorm): GemmaRMSNorm()\n          )\n        )\n        (norm): GemmaRMSNorm()\n      )\n      (lm_head): Linear(in_features=3072, out_features=256000, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ninstruction = \"\"\"Hãy đơn giản hóa câu sau dựa trên thông tin mà tôi cung cấp.\nĐịnh nghĩa: Đơn giản hóa câu bao gồm việc sửa đổi nội dung và cấu trúc của một câu để làm cho nó dễ hiểu hơn, trong khi vẫn giữ lại ý chính và hầu hết nghĩa ban đầu của nó.\nĐể đơn giản hóa một câu, có thể thực hiện một số phép biến đổi dưới đây:\n1. Loại bỏ thông tin không cần thiết.\n2. Loại bỏ hoặc giảm thiểu các cụm từ không mang lại nhiều giá trị thông tin cho câu.\n3. Thay thế các từ / cụm từ phức tạp bằng các từ đồng nghĩa đơn giản hơn.\n4. Chia câu phức tạp, nhiều thông tin thành các câu nhỏ hơn.\nLưu ý rằng câu đơn giản không chứa bất kỳ thông tin nào không có hoặc không đúng với câu gốc.\n\"\"\".strip()\ndef formatting_prompts_func(examples):\n    # instructions = instruction\n    inputs       = examples[\"sentence\"]\n    outputs      = examples[\"result\"]\n    texts = []\n    for input, output in zip(inputs, outputs):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\n\nfrom datasets import load_dataset\ndata_files = {\n    \"train\": \"train.json\",\n    \"eval\": \"eval.json\"\n}\n\ndataset = load_dataset(\"/kaggle/input/ss-dataset\", data_files=data_files)\ndataset = dataset.map(formatting_prompts_func, batched = True,)\ndataset[\"train\"][0]","metadata":{"execution":{"iopub.status.busy":"2024-05-15T01:44:40.843771Z","iopub.execute_input":"2024-05-15T01:44:40.844101Z","iopub.status.idle":"2024-05-15T01:44:40.911857Z","shell.execute_reply.started":"2024-05-15T01:44:40.844074Z","shell.execute_reply":"2024-05-15T01:44:40.910791Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'id': 'difficult_102_5',\n 'result': 'Mọi người đều biết, Soneca và người châu Âu tạo ra \"định mệnh\" và \"thảm họa\". Đối với người Hy Lạp, \"thảm họa\" chỉ là một sự thay đổi, một sự lật ngược. Nói cách khác, nó chỉ là một sự chuyển động vòng tròn, sau đó nó sẽ quay trở lại.',\n 'sentence': 'Ai cũng biết , \" định mệnh \" và \" thảm họa \" là do Soneca và những người châu Âu bịa ra ; với những người Hy Lạp , \" thảm họa \" là bước ngoặt quay ngược lại , đảo xuống phía dưới , dẫu vậy , nó cũng chỉ là \" bước ngoặt \" , có nghĩa , nó là sự chuyển động theo vòng tròn , tới lượt sau nó lại đảo lên phía trên .',\n 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nHãy đơn giản hóa câu sau dựa trên thông tin mà tôi cung cấp.\\nĐịnh nghĩa: Đơn giản hóa câu bao gồm việc sửa đổi nội dung và cấu trúc của một câu để làm cho nó dễ hiểu hơn, trong khi vẫn giữ lại ý chính và hầu hết nghĩa ban đầu của nó.\\nĐể đơn giản hóa một câu, có thể thực hiện một số phép biến đổi dưới đây:\\n1. Loại bỏ thông tin không cần thiết.\\n2. Loại bỏ hoặc giảm thiểu các cụm từ không mang lại nhiều giá trị thông tin cho câu.\\n3. Thay thế các từ / cụm từ phức tạp bằng các từ đồng nghĩa đơn giản hơn.\\n4. Chia câu phức tạp, nhiều thông tin thành các câu nhỏ hơn.\\nLưu ý rằng câu đơn giản không chứa bất kỳ thông tin nào không có hoặc không đúng với câu gốc.\\n\\n### Input:\\nAi cũng biết , \" định mệnh \" và \" thảm họa \" là do Soneca và những người châu Âu bịa ra ; với những người Hy Lạp , \" thảm họa \" là bước ngoặt quay ngược lại , đảo xuống phía dưới , dẫu vậy , nó cũng chỉ là \" bước ngoặt \" , có nghĩa , nó là sự chuyển động theo vòng tròn , tới lượt sau nó lại đảo lên phía trên .\\n\\n### Response:\\nMọi người đều biết, Soneca và người châu Âu tạo ra \"định mệnh\" và \"thảm họa\". Đối với người Hy Lạp, \"thảm họa\" chỉ là một sự thay đổi, một sự lật ngược. Nói cách khác, nó chỉ là một sự chuyển động vòng tròn, sau đó nó sẽ quay trở lại.<eos>'}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_max = -1\nfor i in dataset[\"train\"][\"text\"]:\n    arr = tokenizer.encode(i)\n    length = len(arr)\n    if length > _max:\n        _max = length\nprint(_max)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T14:10:30.290548Z","iopub.execute_input":"2024-05-14T14:10:30.290830Z","iopub.status.idle":"2024-05-14T14:10:36.944840Z","shell.execute_reply.started":"2024-05-14T14:10:30.290807Z","shell.execute_reply":"2024-05-14T14:10:36.943806Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"1017\n","output_type":"stream"}]},{"cell_type":"code","source":"import wandb\nimport os\n\nwandb.login()\n%env WANDB_PROJECT=ss-v1\n%env WANDB_LOG_MODEL=false\n\n# os.environ[\"WANDB_API_KEY\"] = \"764d0f82f6d2b1cbdc302d0e476f0cbddc6a6033\"\n# os.environ[\"WANDB_PROJECT\"]=\"ss-finetune-v2\"","metadata":{"execution":{"iopub.status.busy":"2024-05-15T01:44:40.913105Z","iopub.execute_input":"2024-05-15T01:44:40.913427Z","iopub.status.idle":"2024-05-15T01:44:43.105401Z","shell.execute_reply.started":"2024-05-15T01:44:40.913401Z","shell.execute_reply":"2024-05-15T01:44:43.104196Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhasontung\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"name":"stdout","text":"env: WANDB_PROJECT=ss-v1\nenv: WANDB_LOG_MODEL=false\n","output_type":"stream"}]},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset[\"train\"],\n    eval_dataset = dataset[\"eval\"],\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 4,\n        per_device_eval_batch_size = 1,\n        gradient_accumulation_steps = 16,\n        warmup_steps = 5,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        # max_steps = 60,\n        num_train_epochs=2,\n        learning_rate = 2e-4,\n        fp16 = True,\n        bf16 = False,\n        logging_steps = 1,\n        optim = \"adamw_torch\",\n        lr_scheduler_type = \"constant\",\n        seed = 3407,\n        output_dir = \"gemma7b-r16a32-lr2e-4-adamw32bit-dr0.1\",\n        run_name = \"gemma7b-r16a32-lr2e-4-adamw32bit-dr0.1\",\n        report_to=\"wandb\"\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T01:44:43.107158Z","iopub.execute_input":"2024-05-15T01:44:43.108563Z","iopub.status.idle":"2024-05-15T01:44:44.212095Z","shell.execute_reply.started":"2024-05-15T01:44:43.108520Z","shell.execute_reply":"2024-05-15T01:44:44.211069Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-15T01:44:44.213520Z","iopub.execute_input":"2024-05-15T01:44:44.214244Z","iopub.status.idle":"2024-05-15T10:08:15.231434Z","shell.execute_reply.started":"2024-05-15T01:44:44.214206Z","shell.execute_reply":"2024-05-15T10:08:15.230222Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 9,157 | Num Epochs = 2\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 16\n\\        /    Total batch size = 64 | Total steps = 286\n \"-____-\"     Number of trainable parameters = 50,003,968\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240515_014444-1pgd1mm7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hasontung/ss-v1/runs/1pgd1mm7' target=\"_blank\">magic-deluge-15</a></strong> to <a href='https://wandb.ai/hasontung/ss-v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hasontung/ss-v1' target=\"_blank\">https://wandb.ai/hasontung/ss-v1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hasontung/ss-v1/runs/1pgd1mm7' target=\"_blank\">https://wandb.ai/hasontung/ss-v1/runs/1pgd1mm7</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='286' max='286' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [286/286 8:21:28, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.691500</td>\n      <td>0.579674</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.613500</td>\n      <td>0.587303</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=286, training_loss=0.654458169545327, metrics={'train_runtime': 30210.5789, 'train_samples_per_second': 0.606, 'train_steps_per_second': 0.009, 'total_flos': 3.916238836963369e+17, 'train_loss': 0.654458169545327, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"code","source":"dataset[\"eval\"] = dataset[\"eval\"].shuffle(seed=42)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T10:08:22.662917Z","iopub.execute_input":"2024-05-15T10:08:22.664006Z","iopub.status.idle":"2024-05-15T10:08:22.680912Z","shell.execute_reply.started":"2024-05-15T10:08:22.663937Z","shell.execute_reply":"2024-05-15T10:08:22.679440Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import torch\ndef summarize(model, text, decoding_strategy=\"greedy\"):\n#     FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n    inputs = tokenizer(\n    [\n        alpaca_prompt.format(\n            instruction, # instruction\n            text, # input\n            \"\", # output - leave this blank for generation!\n        )\n    ], return_tensors = \"pt\").to(\"cuda\")\n    inputs_length = len(inputs[\"input_ids\"][0])\n    \n    generation_params = None\n    if decoding_strategy == \"greedy\":\n        generation_params = {\n            \"max_new_tokens\": 300,\n            \"top_k\": 1,\n        }\n    elif decoding_strategy == \"beam\":\n        generation_params = {\n            \"max_new_tokens\": 300,\n            \"num_beams\": 5,\n            \"early_stopping\": True,\n        }\n    elif decoding_strategy == \"sampling\":\n        generation_params = {\n            \"max_new_tokens\": 300,\n            \"do_sample\": True,\n            \"top_p\": 0.95,\n            \"top_k\": 5,\n            \"temperature\": 0.5,\n        }\n    else:\n        print(\"Unknown decoding strategy.\")\n        return None;\n    with torch.inference_mode():\n        outputs = model.generate(\n            **inputs, \n            eos_token_id=tokenizer.eos_token_id,  \n            pad_token_id=tokenizer.pad_token_id,\n            use_cache=True,\n            **generation_params\n        )\n    del inputs\n    torch.cuda.empty_cache()\n    return tokenizer.decode(outputs[0][inputs_length:])","metadata":{"execution":{"iopub.status.busy":"2024-05-15T10:08:23.500893Z","iopub.execute_input":"2024-05-15T10:08:23.501973Z","iopub.status.idle":"2024-05-15T10:08:23.516044Z","shell.execute_reply.started":"2024-05-15T10:08:23.501905Z","shell.execute_reply":"2024-05-15T10:08:23.514625Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"sent = dataset[\"eval\"][\"sentence\"][4]\nprint(alpaca_prompt.format(\n        instruction, # instruction\n        sent, # input\n        \"\", # output - leave this blank for generation!\n    ))\nprint(summarize(model, sent, \"greedy\"))","metadata":{"execution":{"iopub.status.busy":"2024-05-15T10:10:04.086578Z","iopub.execute_input":"2024-05-15T10:10:04.087024Z","iopub.status.idle":"2024-05-15T10:10:10.319070Z","shell.execute_reply.started":"2024-05-15T10:10:04.086990Z","shell.execute_reply":"2024-05-15T10:10:10.314222Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nHãy đơn giản hóa câu sau dựa trên thông tin mà tôi cung cấp.\nĐịnh nghĩa: Đơn giản hóa câu bao gồm việc sửa đổi nội dung và cấu trúc của một câu để làm cho nó dễ hiểu hơn, trong khi vẫn giữ lại ý chính và hầu hết nghĩa ban đầu của nó.\nĐể đơn giản hóa một câu, có thể thực hiện một số phép biến đổi dưới đây:\n1. Loại bỏ thông tin không cần thiết.\n2. Loại bỏ hoặc giảm thiểu các cụm từ không mang lại nhiều giá trị thông tin cho câu.\n3. Thay thế các từ / cụm từ phức tạp bằng các từ đồng nghĩa đơn giản hơn.\n4. Chia câu phức tạp, nhiều thông tin thành các câu nhỏ hơn.\nLưu ý rằng câu đơn giản không chứa bất kỳ thông tin nào không có hoặc không đúng với câu gốc.\n\n### Input:\nNhư vậy , mỗi loại tình cảm nhất định nào đó sẽ đem bóng dáng rõ ràng hoặc âm u tập trung vào quan sát đối với vận động , tập trung vào lực lượng chiêu hoan ( kêu gọi ) , lực lượng này dường như đến từ tự nhiên , lại dường như là chịu ảnh hưởng của chúng ta , nhưng trên thực tế , nó không thuộc về ai , nó là sự liên tiếp giữa người xem và người bị xem .\n\n### Response:\n\nVậy là, mỗi loại tình cảm nào đó sẽ tạo ra một hình ảnh rõ ràng hoặc mờ ám khi quan sát về sự vận động, tập trung vào việc kêu gọi, như thể nó đến từ tự nhiên nhưng cũng như bị ảnh hưởng bởi chúng ta, nhưng thực ra nó không thuộc về ai, mà là sự liên kết giữa người nhìn và người bị nhìn.<eos>\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm import tqdm\nout_data = []\n# length = int(len(test_df) / 3) + 1 if len(test_df) % 3 != 0 else len(test_df) / 3\nwith tqdm(total=len(dataset[\"eval\"][\"sentence\"])) as pbar:\n    for i in range(len(dataset[\"eval\"][\"sentence\"])):\n        greedy_response = summarize(model, dataset[\"eval\"][\"sentence\"][i], \"greedy\").replace(tokenizer.eos_token, \"\").strip()\n#         sampling_response = summarize(model, example.prompt, \"sampling\").replace(\"<eos>\", \"\")\n#         beam_response = summarize(model, example.prompt, \"beam\").replace(\"<eos>\", \"\")\n        temp = {\n            \"sentence\": dataset[\"eval\"][\"sentence\"][i],\n            \"result\": dataset[\"eval\"][\"result\"][i],\n            \"pred\": greedy_response,\n#             \"sampling\": sampling_response,\n#             \"beam\": beam_response,\n        }\n        out_data.append(temp)\n        pbar.update(1)\nprint(out_data[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-15T10:10:51.292727Z","iopub.execute_input":"2024-05-15T10:10:51.293548Z","iopub.status.idle":"2024-05-15T10:40:54.762644Z","shell.execute_reply.started":"2024-05-15T10:10:51.293513Z","shell.execute_reply":"2024-05-15T10:40:54.761317Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"100%|██████████| 500/500 [30:03<00:00,  3.61s/it]","output_type":"stream"},{"name":"stdout","text":"{'sentence': 'Ở phần sau của bài viết , chúng tôi sẽ đi vào trình bày lại một số thực hành để có thể quan sát rõ hơn cách đặt vấn đề và các thao tác trong nghiên cứu .', 'result': 'Trong phần tiếp theo, chúng tôi sẽ giới thiệu một số thực hành giúp hiểu rõ hơn về cách đặt vấn đề và thao tác nghiên cứu.', 'pred': 'Trong phần sau, chúng tôi sẽ nói về một số cách làm để hiểu rõ hơn về việc đặt vấn đề và cách nghiên cứu.'}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"dict_result = {\n    \"training_info\": {\n        \"model\": \"gemma7b\",\n        \"detail\": \"r16 a32 optim torchadamw lr2e-4 dr0.1 2eps\"\n    },\n    \"result\": out_data\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-15T10:40:54.765036Z","iopub.execute_input":"2024-05-15T10:40:54.765465Z","iopub.status.idle":"2024-05-15T10:40:54.772275Z","shell.execute_reply.started":"2024-05-15T10:40:54.765424Z","shell.execute_reply":"2024-05-15T10:40:54.771022Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import json\n\ndef load_json(file_path):\n  with open(file_path, 'r', encoding='utf-8') as json_file:\n    content = json.load(json_file)\n  return content\n\ndef save_json(file_path, data):\n  with open(file_path, 'w', encoding='utf-8') as json_file:\n    json.dump(data, json_file, ensure_ascii=False, indent=2)\n\nsave_json(f\"/kaggle/working/new-ss-gemma7b-r16a32-lr2e-4-adamw32bit-dr0.1\", dict_result)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T10:40:54.774305Z","iopub.execute_input":"2024-05-15T10:40:54.774735Z","iopub.status.idle":"2024-05-15T10:40:54.797792Z","shell.execute_reply.started":"2024-05-15T10:40:54.774684Z","shell.execute_reply":"2024-05-15T10:40:54.796921Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}