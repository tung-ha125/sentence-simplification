{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8398345,"sourceType":"datasetVersion","datasetId":4692054},{"sourceId":8399217,"sourceType":"datasetVersion","datasetId":4626117}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip3 install -q -U bitsandbytes\n!pip3 install -q -U peft\n!pip3 install -q -U trl\n!pip3 install -q -U accelerate\n!pip3 install -q -U datasets==2.17.0\n!pip3 install -q git+https://github.com/huggingface/transformers.git\n!pip install einop","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-14T16:24:57.787050Z","iopub.execute_input":"2024-05-14T16:24:57.787955Z","iopub.status.idle":"2024-05-14T16:28:28.903684Z","shell.execute_reply.started":"2024-05-14T16:24:57.787918Z","shell.execute_reply":"2024-05-14T16:28:28.902393Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# from unsloth import FastLanguageModel\n# import torch\n# max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n# dtype = torch.float16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n# load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n# hf_token = \"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\"","metadata":{"execution":{"iopub.status.busy":"2024-05-14T16:28:28.905677Z","iopub.execute_input":"2024-05-14T16:28:28.906024Z","iopub.status.idle":"2024-05-14T16:28:28.910427Z","shell.execute_reply.started":"2024-05-14T16:28:28.905998Z","shell.execute_reply":"2024-05-14T16:28:28.909554Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nimport os\nimport numpy as np\nfrom peft import LoraConfig, AutoPeftModelForCausalLM\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig, TrainingArguments\nfrom trl import SFTTrainer\n\nmodel_id = \"vinai/PhoGPT-4B\"","metadata":{"execution":{"iopub.status.busy":"2024-05-14T16:28:28.911641Z","iopub.execute_input":"2024-05-14T16:28:28.911913Z","iopub.status.idle":"2024-05-14T16:28:45.420463Z","shell.execute_reply.started":"2024-05-14T16:28:28.911883Z","shell.execute_reply":"2024-05-14T16:28:45.419468Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-05-14 16:28:36.483643: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-14 16:28:36.483760: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-14 16:28:36.604795: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"def create_model_and_tokenizer():\n#     bnb_config = BitsAndBytesConfig(\n#         load_in_4bit=True,\n#         bnb_4bit_use_double_quant=True,\n#         bnb_4bit_quant_type=\"nf4\",\n#         bnb_4bit_compute_dtype=torch.float16\n#     )\n    config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id,\n        config=config,\n#         quantization_config=bnb_config, \n        device_map=\"auto\",\n        torch_dtype=torch.float16,\n#         trust_remote_code=True,\n        load_in_8bit=True\n    )\n    tokenizer.padding_side = \"right\"\n    model.config.use_cache = False\n    return model, tokenizer\n\nmodel, tokenizer = create_model_and_tokenizer()\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-05-14T16:28:45.422381Z","iopub.execute_input":"2024-05-14T16:28:45.422951Z","iopub.status.idle":"2024-05-14T16:29:51.231199Z","shell.execute_reply.started":"2024-05-14T16:28:45.422925Z","shell.execute_reply":"2024-05-14T16:29:51.230276Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69157bdb44444e53a136ee0b80bb61cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_mpt.py:   0%|          | 0.00/16.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d67d1e13c6647c39658944e69917c6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"attention.py:   0%|          | 0.00/24.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cea76022279454bace62e3366d82c2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"norm.py:   0%|          | 0.00/3.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"851195286ee147558d5ccb45cf8d5f32"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- norm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"flash_attn_triton.py:   0%|          | 0.00/28.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0128e3d09a0492c8ba25ea2fc191ffc"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- flash_attn_triton.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"fc.py:   0%|          | 0.00/167 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c9457bc82e849d49f6e1637dd474fba"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- fc.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- attention.py\n- norm.py\n- flash_attn_triton.py\n- fc.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"warnings.py:   0%|          | 0.00/894 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c20f91c4ac8f4091ab9d9b2ee5b605fe"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- warnings.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"blocks.py:   0%|          | 0.00/4.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f1999c7a9af43f486a819760c800c7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ffn.py:   0%|          | 0.00/5.22k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af95f82d675b470993e3a5099dcc3133"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- ffn.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- blocks.py\n- ffn.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- configuration_mpt.py\n- attention.py\n- warnings.py\n- blocks.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n/root/.cache/huggingface/modules/transformers_modules/vinai/PhoGPT-4B/9b1f40c891445071e999aa446ef7303883ee8935/configuration_mpt.py:114: UserWarning: alibi or rope is turned on, setting `learned_pos_emb` to `False.`\n  warnings.warn(f'alibi or rope is turned on, setting `learned_pos_emb` to `False.`')\n/root/.cache/huggingface/modules/transformers_modules/vinai/PhoGPT-4B/9b1f40c891445071e999aa446ef7303883ee8935/configuration_mpt.py:141: UserWarning: If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".\n  warnings.warn(UserWarning('If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".'))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/260 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d80c2ca9241e42ee8cca5c8be2312477"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/844k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ec0275a40004661bd35ef14de72eb98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89207a217389407792154326367f80c3"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for vinai/PhoGPT-4B contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/vinai/PhoGPT-4B.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"modeling_mpt.py:   0%|          | 0.00/32.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f948a14ef3f48cebcc4ec2a54485d57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"custom_embedding.py:   0%|          | 0.00/292 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08cef24616cc482a85b6c1e0652ed92c"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- custom_embedding.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"hf_prefixlm_converter.py:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80c6c94355614af2a22fb5352a4efd98"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- hf_prefixlm_converter.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"param_init_fns.py:   0%|          | 0.00/11.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5deb4371c9df4b78933f0677843166ff"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- param_init_fns.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapt_tokenizer.py:   0%|          | 0.00/1.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edcea40602ec4f869a190420f85ce13d"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- adapt_tokenizer.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"meta_init_context.py:   0%|          | 0.00/3.96k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adf94f4129e949b39869a06b0cf8ee3f"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- meta_init_context.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/vinai/PhoGPT-4B:\n- modeling_mpt.py\n- custom_embedding.py\n- hf_prefixlm_converter.py\n- param_init_fns.py\n- adapt_tokenizer.py\n- meta_init_context.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nThe `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/7.38G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"728e2e422fb3495489d6f19514ff464a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/91.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa04067be5534547ace2cf66c5de84ef"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"MPTForCausalLM(\n  (transformer): MPTModel(\n    (wte): SharedEmbedding(20480, 3072)\n    (emb_drop): Dropout(p=0.0, inplace=False)\n    (blocks): ModuleList(\n      (0-31): 32 x MPTBlock(\n        (norm_1): LPLayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n        (attn): MultiheadAttention(\n          (Wqkv): Linear8bitLt(in_features=3072, out_features=9216, bias=True)\n          (out_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=True)\n        )\n        (norm_2): LPLayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n        (ffn): MPTMLP(\n          (up_proj): Linear8bitLt(in_features=3072, out_features=12288, bias=True)\n          (down_proj): Linear8bitLt(in_features=12288, out_features=3072, bias=True)\n        )\n        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n        (resid_ffn_dropout): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (norm_f): LPLayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"alpaca_prompt = \"\"\"Dưới đây là một hướng dẫn mô tả một nhiệm vụ, kèm theo một đầu vào cung cấp thêm ngữ cảnh. Viết một phản hồi hoàn thành đúng yêu cầu.  \n\n### Hướng dẫn: \n{}  \n\n### Đầu vào: \n{}  \n\n### Phản hồi: \n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ninstruction = \"\"\"Hãy đơn giản hóa câu sau dựa trên thông tin mà tôi cung cấp.\nĐịnh nghĩa: Đơn giản hóa câu bao gồm việc sửa đổi nội dung và cấu trúc của một câu để làm cho nó dễ hiểu hơn, trong khi vẫn giữ lại ý chính và hầu hết nghĩa ban đầu của nó.\nĐể đơn giản hóa một câu, có thể thực hiện một số phép biến đổi dưới đây:\n1. Loại bỏ thông tin không cần thiết.\n2. Loại bỏ hoặc giảm thiểu các cụm từ không mang lại nhiều giá trị thông tin cho câu.\n3. Thay thế các từ / cụm từ phức tạp bằng các từ đồng nghĩa đơn giản hơn.\n4. Chia câu phức tạp, nhiều thông tin thành các câu nhỏ hơn.\nLưu ý rằng câu đơn giản không chứa bất kỳ thông tin nào không có hoặc không đúng với câu gốc.\n\"\"\".strip()\ndef formatting_prompts_func(examples):\n    # instructions = instruction\n    inputs       = examples[\"sentence\"]\n    outputs      = examples[\"result\"]\n    texts = []\n    for input, output in zip(inputs, outputs):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\n\nfrom datasets import load_dataset\ndata_files = {\n    \"train\": \"train.json\",\n    \"eval\": \"eval.json\"\n}\n\ndataset = load_dataset(\"/kaggle/input/ss-dataset\", data_files=data_files)\ndataset = dataset.map(formatting_prompts_func, batched = True,)\ndataset[\"train\"][0]","metadata":{"execution":{"iopub.status.busy":"2024-05-14T16:29:51.232313Z","iopub.execute_input":"2024-05-14T16:29:51.232588Z","iopub.status.idle":"2024-05-14T16:29:56.052971Z","shell.execute_reply.started":"2024-05-14T16:29:51.232564Z","shell.execute_reply":"2024-05-14T16:29:56.052076Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7e94545b8d5422590e1e99f6a955d7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating eval split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b38b726f38846438a8fb7fe6c8edd60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9157 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cc619b274ee4f51b7e743a820f16f22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df36bd85833244b4a9d6d848e9800a47"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'sentence': 'Ai cũng biết , \" định mệnh \" và \" thảm họa \" là do Soneca và những người châu Âu bịa ra ; với những người Hy Lạp , \" thảm họa \" là bước ngoặt quay ngược lại , đảo xuống phía dưới , dẫu vậy , nó cũng chỉ là \" bước ngoặt \" , có nghĩa , nó là sự chuyển động theo vòng tròn , tới lượt sau nó lại đảo lên phía trên .',\n 'id': 'difficult_102_5',\n 'result': 'Mọi người đều biết, Soneca và người châu Âu tạo ra \"định mệnh\" và \"thảm họa\". Đối với người Hy Lạp, \"thảm họa\" chỉ là một sự thay đổi, một sự lật ngược. Nói cách khác, nó chỉ là một sự chuyển động vòng tròn, sau đó nó sẽ quay trở lại.',\n 'text': 'Dưới đây là một hướng dẫn mô tả một nhiệm vụ, kèm theo một đầu vào cung cấp thêm ngữ cảnh. Viết một phản hồi hoàn thành đúng yêu cầu.  \\n\\n### Hướng dẫn: \\nHãy đơn giản hóa câu sau dựa trên thông tin mà tôi cung cấp.\\nĐịnh nghĩa: Đơn giản hóa câu bao gồm việc sửa đổi nội dung và cấu trúc của một câu để làm cho nó dễ hiểu hơn, trong khi vẫn giữ lại ý chính và hầu hết nghĩa ban đầu của nó.\\nĐể đơn giản hóa một câu, có thể thực hiện một số phép biến đổi dưới đây:\\n1. Loại bỏ thông tin không cần thiết.\\n2. Loại bỏ hoặc giảm thiểu các cụm từ không mang lại nhiều giá trị thông tin cho câu.\\n3. Thay thế các từ / cụm từ phức tạp bằng các từ đồng nghĩa đơn giản hơn.\\n4. Chia câu phức tạp, nhiều thông tin thành các câu nhỏ hơn.\\nLưu ý rằng câu đơn giản không chứa bất kỳ thông tin nào không có hoặc không đúng với câu gốc.  \\n\\n### Đầu vào: \\nAi cũng biết , \" định mệnh \" và \" thảm họa \" là do Soneca và những người châu Âu bịa ra ; với những người Hy Lạp , \" thảm họa \" là bước ngoặt quay ngược lại , đảo xuống phía dưới , dẫu vậy , nó cũng chỉ là \" bước ngoặt \" , có nghĩa , nó là sự chuyển động theo vòng tròn , tới lượt sau nó lại đảo lên phía trên .  \\n\\n### Phản hồi: \\nMọi người đều biết, Soneca và người châu Âu tạo ra \"định mệnh\" và \"thảm họa\". Đối với người Hy Lạp, \"thảm họa\" chỉ là một sự thay đổi, một sự lật ngược. Nói cách khác, nó chỉ là một sự chuyển động vòng tròn, sau đó nó sẽ quay trở lại.</s>'}"},"metadata":{}}]},{"cell_type":"code","source":"_max = -1\nfor i in dataset[\"train\"][\"text\"]:\n    arr = tokenizer.encode(i)\n    length = len(arr)\n    if length > _max:\n        _max = length\nprint(_max)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T14:00:42.948795Z","iopub.execute_input":"2024-05-14T14:00:42.949069Z","iopub.status.idle":"2024-05-14T14:00:51.661240Z","shell.execute_reply.started":"2024-05-14T14:00:42.949044Z","shell.execute_reply":"2024-05-14T14:00:51.660314Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"909\n","output_type":"stream"}]},{"cell_type":"code","source":"import wandb\nimport os\n\nwandb.login()\n%env WANDB_PROJECT=ss-v1\n%env WANDB_LOG_MODEL=false\n\n# os.environ[\"WANDB_API_KEY\"] = \"764d0f82f6d2b1cbdc302d0e476f0cbddc6a6033\"\n# os.environ[\"WANDB_PROJECT\"]=\"ss-finetune-v2\"","metadata":{"execution":{"iopub.status.busy":"2024-05-14T16:29:56.054130Z","iopub.execute_input":"2024-05-14T16:29:56.054403Z","iopub.status.idle":"2024-05-14T16:30:07.296594Z","shell.execute_reply.started":"2024-05-14T16:29:56.054380Z","shell.execute_reply":"2024-05-14T16:30:07.295762Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"env: WANDB_PROJECT=ss-v1\nenv: WANDB_LOG_MODEL=false\n","output_type":"stream"}]},{"cell_type":"code","source":"r = 16\na = r * 2\nlora_kwargs = {\n    \"r\": r,\n    \"lora_alpha\": a,\n    \"target_modules\": [\n        \"Wqkv\",\n        \"out_proj\",\n        \"up_proj\",\n        \"down_proj\",\n    ],\n    \"lora_dropout\": 0.1,\n    \"bias\": \"none\",\n    \"task_type\": \"CAUSAL_LM\"\n}\nconfig = LoraConfig(**lora_kwargs)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T16:30:10.028563Z","iopub.execute_input":"2024-05-14T16:30:10.029287Z","iopub.status.idle":"2024-05-14T16:30:10.036692Z","shell.execute_reply.started":"2024-05-14T16:30:10.029257Z","shell.execute_reply":"2024-05-14T16:30:10.035747Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n    model = model,\n    peft_config=config,\n    tokenizer = tokenizer,\n    train_dataset = dataset[\"train\"],\n    eval_dataset = dataset[\"eval\"],\n    dataset_text_field = \"text\",\n    max_seq_length = 1024,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 1,\n        gradient_accumulation_steps = 64,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        # max_steps = 60,\n        num_train_epochs=2,\n        learning_rate = 2e-4,\n#         fp16 = True,\n#         bf16 = False,\n        logging_steps = 1,\n        optim = \"adamw_torch\",\n        lr_scheduler_type = \"constant\",\n        seed = 3407,\n        output_dir = \"phogpt4b-r16a32-lr2e-4-adamw32bit-dr0.1\",\n        run_name = \"phogpt4b-r16a32-lr2e-4-adamw32bit-dr0.1\",\n        report_to=\"wandb\"\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T16:30:12.387929Z","iopub.execute_input":"2024-05-14T16:30:12.388767Z","iopub.status.idle":"2024-05-14T16:30:18.976700Z","shell.execute_reply.started":"2024-05-14T16:30:12.388733Z","shell.execute_reply":"2024-05-14T16:30:18.975663Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/9157 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3efbed616dcc4bc6a446f6761b7b13fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f793d26733614769b3de87cc94983c97"}},"metadata":{}}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T16:30:18.978610Z","iopub.execute_input":"2024-05-14T16:30:18.978918Z","iopub.status.idle":"2024-05-15T00:22:30.349073Z","shell.execute_reply.started":"2024-05-14T16:30:18.978892Z","shell.execute_reply":"2024-05-15T00:22:30.347991Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhasontung\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_163019-zw3cmvbx</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hasontung/ss-v1/runs/zw3cmvbx' target=\"_blank\">phogpt4b-r16a32-lr2e-4-adamw32bit-dr0.1</a></strong> to <a href='https://wandb.ai/hasontung/ss-v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hasontung/ss-v1' target=\"_blank\">https://wandb.ai/hasontung/ss-v1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hasontung/ss-v1/runs/zw3cmvbx' target=\"_blank\">https://wandb.ai/hasontung/ss-v1/runs/zw3cmvbx</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/root/.cache/huggingface/modules/transformers_modules/vinai/PhoGPT-4B/9b1f40c891445071e999aa446ef7303883ee8935/configuration_mpt.py:114: UserWarning: alibi or rope is turned on, setting `learned_pos_emb` to `False.`\n  warnings.warn(f'alibi or rope is turned on, setting `learned_pos_emb` to `False.`')\n/root/.cache/huggingface/modules/transformers_modules/vinai/PhoGPT-4B/9b1f40c891445071e999aa446ef7303883ee8935/configuration_mpt.py:141: UserWarning: If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".\n  warnings.warn(UserWarning('If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".'))\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/root/.cache/huggingface/modules/transformers_modules/vinai/PhoGPT-4B/9b1f40c891445071e999aa446ef7303883ee8935/attention.py:87: UserWarning: Propagating key_padding_mask to the attention module and applying it within the attention module can cause unnecessary computation/memory usage. Consider integrating into attn_bias once and passing that to each attention module instead.\n  warnings.warn('Propagating key_padding_mask to the attention module ' + 'and applying it within the attention module can cause ' + 'unnecessary computation/memory usage. Consider integrating ' + 'into attn_bias once and passing that to each attention ' + 'module instead.')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='286' max='286' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [286/286 7:50:04, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.647600</td>\n      <td>0.557921</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.624300</td>\n      <td>0.545048</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/root/.cache/huggingface/modules/transformers_modules/vinai/PhoGPT-4B/9b1f40c891445071e999aa446ef7303883ee8935/attention.py:87: UserWarning: Propagating key_padding_mask to the attention module and applying it within the attention module can cause unnecessary computation/memory usage. Consider integrating into attn_bias once and passing that to each attention module instead.\n  warnings.warn('Propagating key_padding_mask to the attention module ' + 'and applying it within the attention module can cause ' + 'unnecessary computation/memory usage. Consider integrating ' + 'into attn_bias once and passing that to each attention ' + 'module instead.')\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/root/.cache/huggingface/modules/transformers_modules/vinai/PhoGPT-4B/9b1f40c891445071e999aa446ef7303883ee8935/configuration_mpt.py:114: UserWarning: alibi or rope is turned on, setting `learned_pos_emb` to `False.`\n  warnings.warn(f'alibi or rope is turned on, setting `learned_pos_emb` to `False.`')\n/root/.cache/huggingface/modules/transformers_modules/vinai/PhoGPT-4B/9b1f40c891445071e999aa446ef7303883ee8935/configuration_mpt.py:141: UserWarning: If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".\n  warnings.warn(UserWarning('If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".'))\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=286, training_loss=0.7191455841481269, metrics={'train_runtime': 28330.9335, 'train_samples_per_second': 0.646, 'train_steps_per_second': 0.01, 'total_flos': 1.3574178278693683e+17, 'train_loss': 0.7191455841481269, 'epoch': 1.9989079392814242})"},"metadata":{}}]},{"cell_type":"code","source":"dataset[\"eval\"] = dataset[\"eval\"].shuffle(seed=42)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T00:22:39.604255Z","iopub.execute_input":"2024-05-15T00:22:39.604614Z","iopub.status.idle":"2024-05-15T00:22:39.617383Z","shell.execute_reply.started":"2024-05-15T00:22:39.604584Z","shell.execute_reply":"2024-05-15T00:22:39.616508Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import torch\ndef summarize(model, text, decoding_strategy=\"greedy\"):\n#     FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n    inputs = tokenizer(\n    [\n        alpaca_prompt.format(\n            instruction, # instruction\n            text, # input\n            \"\", # output - leave this blank for generation!\n        )\n    ], return_tensors = \"pt\").to(\"cuda\")\n    inputs_length = len(inputs[\"input_ids\"][0])\n    \n    generation_params = None\n    if decoding_strategy == \"greedy\":\n        generation_params = {\n            \"max_new_tokens\": 300,\n            \"top_k\": 1,\n        }\n    elif decoding_strategy == \"beam\":\n        generation_params = {\n            \"max_new_tokens\": 300,\n            \"num_beams\": 5,\n            \"early_stopping\": True,\n        }\n    elif decoding_strategy == \"sampling\":\n        generation_params = {\n            \"max_new_tokens\": 300,\n            \"do_sample\": True,\n            \"top_p\": 0.95,\n            \"top_k\": 5,\n            \"temperature\": 0.5,\n        }\n    else:\n        print(\"Unknown decoding strategy.\")\n        return None;\n    with torch.inference_mode():\n        outputs = model.generate(\n            **inputs, \n            eos_token_id=tokenizer.eos_token_id,  \n            pad_token_id=tokenizer.pad_token_id,\n            use_cache=True,\n            **generation_params\n        )\n    del inputs\n    torch.cuda.empty_cache()\n    return tokenizer.decode(outputs[0][inputs_length:])","metadata":{"execution":{"iopub.status.busy":"2024-05-15T00:22:39.619225Z","iopub.execute_input":"2024-05-15T00:22:39.619557Z","iopub.status.idle":"2024-05-15T00:22:39.630685Z","shell.execute_reply.started":"2024-05-15T00:22:39.619522Z","shell.execute_reply":"2024-05-15T00:22:39.629583Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"sent = dataset[\"eval\"][\"sentence\"][200]\nprint(alpaca_prompt.format(\n        instruction, # instruction\n        sent, # input\n        \"\", # output - leave this blank for generation!\n    ))\nprint(summarize(model, sent, \"greedy\"))","metadata":{"execution":{"iopub.status.busy":"2024-05-15T00:24:07.236308Z","iopub.execute_input":"2024-05-15T00:24:07.236671Z","iopub.status.idle":"2024-05-15T00:24:13.163897Z","shell.execute_reply.started":"2024-05-15T00:24:07.236641Z","shell.execute_reply":"2024-05-15T00:24:13.162710Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Dưới đây là một hướng dẫn mô tả một nhiệm vụ, kèm theo một đầu vào cung cấp thêm ngữ cảnh. Viết một phản hồi hoàn thành đúng yêu cầu.  \n\n### Hướng dẫn: \nHãy đơn giản hóa câu sau dựa trên thông tin mà tôi cung cấp.\nĐịnh nghĩa: Đơn giản hóa câu bao gồm việc sửa đổi nội dung và cấu trúc của một câu để làm cho nó dễ hiểu hơn, trong khi vẫn giữ lại ý chính và hầu hết nghĩa ban đầu của nó.\nĐể đơn giản hóa một câu, có thể thực hiện một số phép biến đổi dưới đây:\n1. Loại bỏ thông tin không cần thiết.\n2. Loại bỏ hoặc giảm thiểu các cụm từ không mang lại nhiều giá trị thông tin cho câu.\n3. Thay thế các từ / cụm từ phức tạp bằng các từ đồng nghĩa đơn giản hơn.\n4. Chia câu phức tạp, nhiều thông tin thành các câu nhỏ hơn.\nLưu ý rằng câu đơn giản không chứa bất kỳ thông tin nào không có hoặc không đúng với câu gốc.  \n\n### Đầu vào: \nThỉnh thoảng cũng có một vài hoàng tử nghe kể về câu chuyện truyền thuyết ấy đã chui vào bụi hồng gai tìm cách vào lâu đài , nhưng bụi gai như có tay giữ chặt họ lại , khiến họ bị mắc nghẽn .  \n\n### Phản hồi: \n\nCó một số hoàng tử nghe kể về câu chuyện này và muốn vào lâu đài, nhưng bụi gai giữ họ lại, khiến họ bị mắc kẹt.</s>\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm import tqdm\nout_data = []\n# length = int(len(test_df) / 3) + 1 if len(test_df) % 3 != 0 else len(test_df) / 3\nwith tqdm(total=len(dataset[\"eval\"][\"sentence\"])) as pbar:\n    for i in range(len(dataset[\"eval\"][\"sentence\"])):\n        greedy_response = summarize(model, dataset[\"eval\"][\"sentence\"][i], \"greedy\").replace(tokenizer.eos_token, \"\").strip()\n#         sampling_response = summarize(model, example.prompt, \"sampling\").replace(\"<eos>\", \"\")\n#         beam_response = summarize(model, example.prompt, \"beam\").replace(\"<eos>\", \"\")\n        temp = {\n            \"sentence\": dataset[\"eval\"][\"sentence\"][i],\n            \"result\": dataset[\"eval\"][\"result\"][i],\n            \"pred\": greedy_response,\n#             \"sampling\": sampling_response,\n#             \"beam\": beam_response,\n        }\n        out_data.append(temp)\n        pbar.update(1)\nprint(out_data[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-15T00:24:29.865672Z","iopub.execute_input":"2024-05-15T00:24:29.866506Z","iopub.status.idle":"2024-05-15T01:28:53.400771Z","shell.execute_reply.started":"2024-05-15T00:24:29.866474Z","shell.execute_reply":"2024-05-15T01:28:53.399663Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"100%|██████████| 500/500 [1:04:23<00:00,  7.73s/it]","output_type":"stream"},{"name":"stdout","text":"{'sentence': 'Ở phần sau của bài viết , chúng tôi sẽ đi vào trình bày lại một số thực hành để có thể quan sát rõ hơn cách đặt vấn đề và các thao tác trong nghiên cứu .', 'result': 'Trong phần tiếp theo, chúng tôi sẽ giới thiệu một số thực hành giúp hiểu rõ hơn về cách đặt vấn đề và thao tác nghiên cứu.', 'pred': 'Ở phần sau, chúng tôi sẽ nói về cách đặt vấn đề và cách làm việc trong nghiên cứu.'}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"dict_result = {\n    \"training_info\": {\n        \"model\": \"phogpt4b\",\n        \"detail\": \"r16 a32 optim torchadamw lr2e-4 dr0.1 2eps\"\n    },\n    \"result\": out_data\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-15T01:28:53.402935Z","iopub.execute_input":"2024-05-15T01:28:53.403355Z","iopub.status.idle":"2024-05-15T01:28:53.409174Z","shell.execute_reply.started":"2024-05-15T01:28:53.403320Z","shell.execute_reply":"2024-05-15T01:28:53.408153Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import json\n\ndef load_json(file_path):\n  with open(file_path, 'r', encoding='utf-8') as json_file:\n    content = json.load(json_file)\n  return content\n\ndef save_json(file_path, data):\n  with open(file_path, 'w', encoding='utf-8') as json_file:\n    json.dump(data, json_file, ensure_ascii=False, indent=2)\n\nsave_json(f\"/kaggle/working/new-ss-phogpt4b-r16a32-lr2e-4-adamw32bit-dr0.1\", dict_result)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T01:28:53.410323Z","iopub.execute_input":"2024-05-15T01:28:53.410586Z","iopub.status.idle":"2024-05-15T01:28:53.432498Z","shell.execute_reply.started":"2024-05-15T01:28:53.410564Z","shell.execute_reply":"2024-05-15T01:28:53.431200Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}